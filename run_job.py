import subprocess
import sys
import os
import pandas as pd
from datetime import datetime
import time

# í¬ë¡¤ëŸ¬ ì •ë³´
CRAWLERS = {
    'GSC': {'script': 'gsc_crawler.py', 'filename': 'gsc_greenbean.xlsx'},
    'ë§¥ë„í‹°': {'script': 'ë§¥ë„í‹°_í¬ë¡¤ëŸ¬.py', 'filename': 'ë§¥ë„í‹°_ì›ë‘.xlsx'},
    'ë¸”ë ˆìŠ¤ë¹ˆ': {'script': 'ë¸”ë ˆìŠ¤ë¹ˆ_í¬ë¡¤ëŸ¬.py', 'filename': 'ë¸”ë ˆìŠ¤ë¹ˆ_ì›ë‘.xlsx'},
    'ì— ì•„ì´ì»¤í”¼': {'script': 'ì— ì•„ì´ì»¤í”¼_í¬ë¡¤ëŸ¬.py', 'filename': 'ì— ì•„ì´ì»¤í”¼_ì›ë‘.xlsx'},
    'ì½”ë¹ˆì¦ˆ': {'script': 'ì½”ë¹ˆì¦ˆ_í¬ë¡¤ëŸ¬.py', 'filename': 'ì½”ë¹ˆì¦ˆ_ì›ë‘.xlsx'},
    'ì•Œë§ˆì”¨ì—˜ë¡œ': {'script': 'ì•Œë§ˆì”¨ì—˜ë¡œ_í¬ë¡¤ëŸ¬.py', 'filename': 'ì•Œë§ˆì”¨ì—˜ë¡œ_ì›ë‘.xlsx'},
    'ë¦¬ë¸Œë ˆ': {'script': 'ë¦¬ë¸Œë ˆ_í¬ë¡¤ëŸ¬.py', 'filename': 'ë¦¬ë¸Œë ˆ_ì›ë‘.xlsx'},
    'í›„ì„±': {'script': 'í›„ì„±_í¬ë¡¤ëŸ¬.py', 'filename': 'í›„ì„±_ì›ë‘.xlsx'},
    'ëª¨ëª¨ìŠ¤': {'script': 'ëª¨ëª¨ìŠ¤_í¬ë¡¤ëŸ¬.py', 'filename': 'ëª¨ëª¨ìŠ¤_ì›ë‘.xlsx'}
}

def log_message(message):
    """ì½˜ì†”ì— ë¡œê·¸ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}")

def run_single_crawler(crawler_name):
    """ë‹¨ì¼ í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•˜ê³  ì„±ê³µ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        script_name = CRAWLERS[crawler_name]['script']
        log_message(f"--- Executing: {crawler_name} ({script_name}) ---")
        
        # í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ íŒŒì´ì¬ìœ¼ë¡œ ì§ì ‘ ì‹¤í–‰
        result = subprocess.run(
            [sys.executable, script_name],
            capture_output=True,
            text=True,
            encoding='utf-8',
            timeout=1800  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
        )

        if result.returncode == 0:
            log_message(f"âœ… Success: {crawler_name} finished.")
            print(result.stdout) # í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ì˜ printë¬¸ ì¶œë ¥
            return True
        else:
            log_message(f"âŒ ERROR: {crawler_name} failed with return code {result.returncode}.")
            print("--- stdout ---")
            print(result.stdout)
            print("--- stderr ---")
            print(result.stderr)
            return False
            
    except Exception as e:
        log_message(f"âŒ EXCEPTION during {crawler_name} execution: {e}")
        return False

def merge_excel_files():
    """ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ê²°ê³¼ ì—‘ì…€ íŒŒì¼ì„ í•˜ë‚˜ë¡œ í†µí•©í•©ë‹ˆë‹¤."""
    log_message("--- Merging all excel files... ---")
    all_data = []
    
    for name, info in CRAWLERS.items():
        filename = info['filename']
        if os.path.exists(filename):
            try:
                df = pd.read_excel(filename, skiprows=1)
                all_data.append(df)
                log_message(f"-> Merged: {filename} ({len(df)} rows)")
            except Exception as e:
                log_message(f"-> Failed to read {filename}: {e}")
                continue
    
    if all_data:
        merged_df = pd.concat(all_data, ignore_index=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"í†µí•©_ìƒë‘_ë°ì´í„°_{timestamp}.xlsx"
        
        with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:
            now_str = datetime.now().strftime("í†µí•© í¬ë¡¤ë§ ì¼ì‹œ: %Y-%m-%d %H:%M:%S")
            pd.DataFrame([[now_str]]).to_excel(writer, index=False, header=False)
            merged_df.to_excel(writer, index=False, startrow=1)
        
        log_message(f"âœ… Successfully created í†µí•© ì—‘ì…€ íŒŒì¼: {output_filename} ({len(merged_df)} total rows)")
        return output_filename
    else:
        log_message("âŒ No data to merge.")
        return None

def main_job():
    """ì „ì²´ í¬ë¡¤ë§ ì‘ì—…ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    log_message("ğŸš€ Starting Daily Green Bean Crawler Job")
    start_time = time.time()
    
    successful_crawlers = 0
    total_crawlers = len(CRAWLERS)
    
    for name in CRAWLERS.keys():
        if run_single_crawler(name):
            successful_crawlers += 1
        # ì„œë²„ì— ë¶€ë‹´ì„ ì£¼ì§€ ì•Šë„ë¡ í¬ë¡¤ëŸ¬ ì‚¬ì´ì— ì•½ê°„ì˜ ë”œë ˆì´ë¥¼ ë‘¡ë‹ˆë‹¤.
        time.sleep(5)
        
    log_message(f"--- Job Summary: {successful_crawlers} out of {total_crawlers} crawlers succeeded. ---")
    
    if successful_crawlers > 0:
        merge_excel_files()
        # ì—¬ê¸°ì— ë‚˜ì¤‘ì— í†µí•© íŒŒì¼ì„ êµ¬ê¸€ ë“œë¼ì´ë¸Œë‚˜ ë‹¤ë¥¸ ê³³ì— ì—…ë¡œë“œí•˜ëŠ” ì½”ë“œë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    end_time = time.time()
    elapsed = end_time - start_time
    log_message(f"ğŸ Finished Daily Crawler Job in {elapsed:.2f} seconds.")

if __name__ == "__main__":
    main_job() 