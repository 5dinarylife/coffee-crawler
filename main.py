from flask import Flask, render_template, request, jsonify, send_file
import subprocess
import threading
import time
import os
import json
from datetime import datetime
import tempfile
import zipfile
import io
import pandas as pd

app = Flask(__name__)

# í¬ë¡¤ëŸ¬ ì •ë³´ ì •ì˜
CRAWLERS = {
    'GSC': {'script': 'gsc_crawler.py', 'filename': 'gsc_greenbean.xlsx', 'display_name': 'GSC'},
    'ë§¥ë„í‹°': {'script': 'ë§¥ë„í‹°_í¬ë¡¤ëŸ¬.py', 'filename': 'ë§¥ë„í‹°_ì›ë‘.xlsx', 'display_name': 'ë§¥ë„í‹°'},
    'ë¸”ë ˆìŠ¤ë¹ˆ': {'script': 'ë¸”ë ˆìŠ¤ë¹ˆ_í¬ë¡¤ëŸ¬.py', 'filename': 'ë¸”ë ˆìŠ¤ë¹ˆ_ì›ë‘.xlsx', 'display_name': 'ë¸”ë ˆìŠ¤ë¹ˆ'},
    'ì— ì•„ì´ì»¤í”¼': {'script': 'ì— ì•„ì´ì»¤í”¼_í¬ë¡¤ëŸ¬.py', 'filename': 'ì— ì•„ì´ì»¤í”¼_ì›ë‘.xlsx', 'display_name': 'ì— ì•„ì´ì»¤í”¼'},
    'ì½”ë¹ˆì¦ˆ': {'script': 'ì½”ë¹ˆì¦ˆ_í¬ë¡¤ëŸ¬.py', 'filename': 'ì½”ë¹ˆì¦ˆ_ì›ë‘.xlsx', 'display_name': 'ì½”ë¹ˆì¦ˆ'},
    'ì•Œë§ˆì”¨ì—˜ë¡œ': {'script': 'ì•Œë§ˆì”¨ì—˜ë¡œ_í¬ë¡¤ëŸ¬.py', 'filename': 'ì•Œë§ˆì”¨ì—˜ë¡œ_ì›ë‘.xlsx', 'display_name': 'ì•Œë§ˆì”¨ì—˜ë¡œ'},
    'ë¦¬ë¸Œë ˆ': {'script': 'ë¦¬ë¸Œë ˆ_í¬ë¡¤ëŸ¬.py', 'filename': 'ë¦¬ë¸Œë ˆ_ì›ë‘.xlsx', 'display_name': 'ë¦¬ë¸Œë ˆ'},
    'í›„ì„±': {'script': 'í›„ì„±_í¬ë¡¤ëŸ¬.py', 'filename': 'í›„ì„±_ì›ë‘.xlsx', 'display_name': 'í›„ì„±'},
    'ëª¨ëª¨ìŠ¤': {'script': 'ëª¨ëª¨ìŠ¤_í¬ë¡¤ëŸ¬.py', 'filename': 'ëª¨ëª¨ìŠ¤_ì›ë‘.xlsx', 'display_name': 'ëª¨ëª¨ìŠ¤'}
}

# ì „ì—­ ë³€ìˆ˜ë¡œ í¬ë¡¤ë§ ìƒíƒœ ê´€ë¦¬
crawling_status = {
    'is_running': False,
    'start_time': None,
    'end_time': None,
    'logs': [],
    'progress': 0,
    'total_crawlers': len(CRAWLERS),
    'completed_crawlers': 0,
    'individual_status': {}
}

# ê°œë³„ í¬ë¡¤ëŸ¬ ìƒíƒœ ì´ˆê¸°í™”
for crawler_name in CRAWLERS.keys():
    crawling_status['individual_status'][crawler_name] = {
        'status': 'idle',  # idle, running, completed, failed
        'start_time': None,
        'end_time': None,
        'duration': None,
        'data_count': 0,
        'logs': [],
        'error': None
    }

def log_message(message, crawler_name=None, level=None):
    """
    ë¡œê·¸ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ê³  íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. level: 'info', 'error', 'warning' ë“±
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"[{timestamp}] {message}"
    if level == 'error':
        log_entry = f"[ERROR] {log_entry}"
    elif level == 'warning':
        log_entry = f"[WARN] {log_entry}"
    # ê°œë³„ í¬ë¡¤ëŸ¬ ë¡œê·¸
    if crawler_name:
        if crawler_name in crawling_status['individual_status']:
            crawling_status['individual_status'][crawler_name]['logs'].append(log_entry)
    else:
        crawling_status['logs'].append(log_entry)
    print(log_entry)

def get_data_count(filename):
    """ì—‘ì…€ íŒŒì¼ì˜ ë°ì´í„° í–‰ ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        if os.path.exists(filename):
            df = pd.read_excel(filename, skiprows=1)
            return len(df)
        return 0
    except Exception as e:
        print(f"ë°ì´í„° ìˆ˜ ê³„ì‚° ì˜¤ë¥˜ ({filename}): {e}")
        return 0

def run_single_crawler(crawler_name):
    """ë‹¨ì¼ í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜"""
    global crawling_status
    
    if crawler_name not in CRAWLERS:
        log_message(f"ì•Œ ìˆ˜ ì—†ëŠ” í¬ë¡¤ëŸ¬: {crawler_name}", crawler_name, level='error')
        return
    
    # ìƒíƒœ ì´ˆê¸°í™”
    status = crawling_status['individual_status'][crawler_name]
    status['status'] = 'running'
    status['start_time'] = datetime.now().isoformat()
    status['end_time'] = None
    status['duration'] = None
    status['data_count'] = 0
    status['logs'] = []
    status['error'] = None
    
    script_name = CRAWLERS[crawler_name]['script']
    filename = CRAWLERS[crawler_name]['filename']
    
    log_message(f"'{crawler_name}' ë²„íŠ¼ì´ ëˆŒë ¸ë‹¤.", crawler_name)
    log_message(f"--- {crawler_name} í¬ë¡¤ëŸ¬ ì‹œì‘ ---", crawler_name)
    
    try:
        start_time = time.time()
        
        # í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        result = subprocess.run(
            ['python', script_name],
            capture_output=True,
            text=True,
            encoding='utf-8',
            timeout=1800  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        if result.returncode == 0:
            # ì„±ê³µ
            status['status'] = 'completed'
            status['duration'] = round(duration, 2)
            status['data_count'] = get_data_count(filename)
            
            log_message(f"âœ… {crawler_name} í¬ë¡¤ë§ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ, ë°ì´í„°: {status['data_count']}ê°œ)", crawler_name)
            
            if result.stdout:
                log_message(f"ì¶œë ¥: {result.stdout}", crawler_name)
        else:
            # ì‹¤íŒ¨
            status['status'] = 'failed'
            status['duration'] = round(duration, 2)
            status['error'] = result.stderr
            
            log_message(f"âŒ {crawler_name} í¬ë¡¤ë§ ì‹¤íŒ¨(ì½”ë“œ: {result.returncode})", crawler_name, level='error')
            log_message(f"ì˜¤ë¥˜: {result.stderr}", crawler_name, level='error')
            # ì „ì²´ ì‹¤í–‰ ë¡œê·¸ì—ë„ Traceback ë‚¨ê¸°ê¸°
            log_message(f"[{crawler_name}] Traceback:\n{result.stderr}", None, level='error')
            
    except subprocess.TimeoutExpired:
        status['status'] = 'failed'
        status['error'] = 'ì‹œê°„ ì´ˆê³¼ (30ë¶„)'
        log_message(f"â° {crawler_name} í¬ë¡¤ë§ ì‹œê°„ ì´ˆê³¼", crawler_name, level='error')
    except Exception as e:
        status['status'] = 'failed'
        status['error'] = str(e)
        log_message(f"âŒ {crawler_name} í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}", crawler_name, level='error')
    finally:
        status['end_time'] = datetime.now().isoformat()
        if status['status'] == 'failed':
            log_message(f"'{crawler_name}' í¬ë¡¤ë§ ì‹¤íŒ¨({status['error']})", crawler_name, level='error')

def run_all_crawlers():
    """ëª¨ë“  í¬ë¡¤ëŸ¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜"""
    global crawling_status
    
    try:
        crawling_status['is_running'] = True
        crawling_status['start_time'] = datetime.now().isoformat()
        crawling_status['logs'] = []
        crawling_status['progress'] = 0
        crawling_status['completed_crawlers'] = 0
        
        log_message("ì „ì²´ í¬ë¡¤ë§ ë²„íŠ¼ì´ ëˆŒë ¸ë‹¤.")
        log_message("ğŸš€ ì „ì²´ í¬ë¡¤ë§ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        total_crawlers = len(CRAWLERS)
        completed = 0
        
        for crawler_name in CRAWLERS.keys():
            if not crawling_status['is_running']:  # ì¤‘ë‹¨ ì²´í¬
                break
                
            run_single_crawler(crawler_name)
            completed += 1
            crawling_status['completed_crawlers'] = completed
            crawling_status['progress'] = int((completed / total_crawlers) * 100)
            
            # í¬ë¡¤ëŸ¬ ê°„ ë”œë ˆì´
            time.sleep(2)
        
        if crawling_status['is_running']:
            log_message("âœ… ì „ì²´ í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            crawling_status['progress'] = 100
            
    except Exception as e:
        log_message(f"âŒ ì „ì²´ í¬ë¡¤ë§ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", level='error')
    finally:
        crawling_status['is_running'] = False
        crawling_status['end_time'] = datetime.now().isoformat()

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template('index.html', crawlers=CRAWLERS)

@app.route('/start', methods=['POST'])
def start_crawling():
    """ì „ì²´ í¬ë¡¤ë§ ì‘ì—… ì‹œì‘"""
    if crawling_status['is_running']:
        return jsonify({'status': 'error', 'message': 'ì´ë¯¸ í¬ë¡¤ë§ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.'})
    
    # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
    thread = threading.Thread(target=run_all_crawlers)
    thread.daemon = True
    thread.start()
    
    return jsonify({'status': 'success', 'message': 'ì „ì²´ í¬ë¡¤ë§ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.'})

@app.route('/start_single/<crawler_name>', methods=['POST'])
def start_single_crawler(crawler_name):
    """ê°œë³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    if crawler_name not in CRAWLERS:
        return jsonify({'status': 'error', 'message': f'ì•Œ ìˆ˜ ì—†ëŠ” í¬ë¡¤ëŸ¬: {crawler_name}'})
    
    status = crawling_status['individual_status'][crawler_name]
    if status['status'] == 'running':
        return jsonify({'status': 'error', 'message': f'{crawler_name} í¬ë¡¤ëŸ¬ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.'})
    
    # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ê°œë³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰
    thread = threading.Thread(target=run_single_crawler, args=(crawler_name,))
    thread.daemon = True
    thread.start()
    
    return jsonify({'status': 'success', 'message': f'{crawler_name} í¬ë¡¤ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.'})

@app.route('/stop', methods=['POST'])
def stop_crawling():
    """í¬ë¡¤ë§ ì‘ì—… ì¤‘ë‹¨"""
    if not crawling_status['is_running']:
        return jsonify({'status': 'error', 'message': 'ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ë§ì´ ì—†ìŠµë‹ˆë‹¤.'})
    
    crawling_status['is_running'] = False
    log_message("ğŸ›‘ í¬ë¡¤ë§ ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return jsonify({'status': 'success', 'message': 'í¬ë¡¤ë§ ì‘ì—…ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.'})

@app.route('/status')
def get_status():
    """í¬ë¡¤ë§ ìƒíƒœ ì¡°íšŒ"""
    return jsonify(crawling_status)

@app.route('/download')
def download_results():
    """í†µí•© í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        # í†µí•© ì—‘ì…€ íŒŒì¼ë“¤ ì°¾ê¸°
        excel_files = []
        for file in os.listdir('.'):
            if file.endswith('.xlsx') and 'í†µí•©' in file:
                excel_files.append(file)
        
        if not excel_files:
            return jsonify({'status': 'error', 'message': 'ë‹¤ìš´ë¡œë“œí•  í†µí•© íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'})
        
        # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
        latest_file = max(excel_files, key=os.path.getctime)
        
        return send_file(
            latest_file,
            as_attachment=True,
            download_name=latest_file,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
        
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}'})

@app.route('/download_single/<crawler_name>')
def download_single_result(crawler_name):
    """ê°œë³„ í¬ë¡¤ëŸ¬ ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    try:
        if crawler_name not in CRAWLERS:
            return jsonify({'status': 'error', 'message': f'ì•Œ ìˆ˜ ì—†ëŠ” í¬ë¡¤ëŸ¬: {crawler_name}'})
        
        filename = CRAWLERS[crawler_name]['filename']
        
        if not os.path.exists(filename):
            return jsonify({'status': 'error', 'message': f'{crawler_name}ì˜ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'})
        
        return send_file(
            filename,
            as_attachment=True,
            download_name=filename,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
        
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}'})

@app.route('/download-all')
def download_all_files():
    """ëª¨ë“  ì—‘ì…€ íŒŒì¼ì„ ZIPìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ"""
    try:
        # ì—‘ì…€ íŒŒì¼ë“¤ ì°¾ê¸°
        excel_files = [f for f in os.listdir('.') if f.endswith('.xlsx')]
        
        if not excel_files:
            return jsonify({'status': 'error', 'message': 'ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'})
        
        # ZIP íŒŒì¼ ìƒì„±
        memory_file = io.BytesIO()
        with zipfile.ZipFile(memory_file, 'w') as zf:
            for file in excel_files:
                zf.write(file, file)
        
        memory_file.seek(0)
        
        return send_file(
            memory_file,
            as_attachment=True,
            download_name=f'coffee_crawler_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.zip',
            mimetype='application/zip'
        )
        
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}'})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 8080)), debug=False) 